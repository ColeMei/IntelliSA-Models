# Generative Model Configuration
model_name: "Qwen/Qwen2.5-Coder-7B"
max_length: 512
# Prompt formatting: "llama" or "qwen" (auto-detected from model_name if omitted)
prompt_style: "qwen"
batch_size: 1
learning_rate: 5e-5
num_epochs: 3
warmup_steps: 100
eval_steps: 50
save_steps: 100

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

# Memory optimization
use_4bit: true # Enable 4-bit quantization for memory efficiency
gradient_accumulation_steps: 4 # Accumulate gradients for effective batch size

# Data paths (will be overridden by CLI args)
train_path: "data/processed/train.jsonl"
val_path: "data/processed/val.jsonl"
output_dir: "models/generative"

# Training arguments
evaluation_strategy: "steps"
save_strategy: "steps"
logging_steps: 10
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
fp16: true
weight_decay: 0.01
