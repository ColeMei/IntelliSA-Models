# Generative Model Configuration
model_name: "codellama/CodeLlama-34b-hf"
max_length: 512
batch_size: 1
learning_rate: 5e-5
num_epochs: 3
warmup_steps: 100
eval_steps: 50
save_steps: 100

# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Memory optimization
use_4bit: true # Enable 4-bit quantization for memory efficiency
gradient_accumulation_steps: 4 # Accumulate gradients for effective batch size

# Data paths (will be overridden by CLI args)
train_path: "data/processed/train.jsonl"
val_path: "data/processed/val.jsonl"
output_dir: "models/generative"

# Training arguments
evaluation_strategy: "steps"
save_strategy: "steps"
logging_steps: 10
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
fp16: true
weight_decay: 0.01
