# Batch training configuration for generative models hyperparameter sweep
# This config defines all generative experiments to run (placeholder for future use)

# Global settings applied to all experiments
global:
  max_length: 256
  warmup_steps: 100
  eval_steps: 50
  save_steps: 100
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 10
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  train_path: "data/processed/chef_train.jsonl"
  val_path: "data/processed/chef_val.jsonl"

# Generative experiments (LoRA fine-tuning) - placeholder for future use
experiments:
  # codellama7b:
  #   model_name: "codellama/CodeLlama-7b-hf"
  #   hyperparameters:
  #     learning_rates: [1e-5, 2e-5, 5e-5]
  #     batch_sizes: [1, 2]
  #     num_epochs: [3]
  #     lora_ranks: [8, 16]

# Output configuration
output:
  base_dir: "models/experiments/generative"
  results_dir: "results/experiments/generative"
  configs_dir: "configs/experiments/generative"
  naming_pattern: "{model}_lora{lora_rank}_lr{learning_rate}_bs{batch_size}_ep{epochs}_{timestamp}_job{id}"

# SLURM configuration
slurm:
  partition: "gpu-a100"
  qos: "normal"
  gres: "gpu:1"
  cpus_per_task: 4
  mem: "32G"
  time: "0-4:00:00"
  tmp: "30GB"
