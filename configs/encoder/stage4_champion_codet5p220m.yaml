# Champion batch training configuration for codet5p-220m
# Purpose: run stable multiple seeds on the champion hyperparameters

global:
  lr_scheduler_type: "cosine"
  warmup_steps: 500

experiments:
  codet5p_220m_champion:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      learning_rates: [4e-5]
      batch_sizes: [8]
      num_epochs: [6]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
      seeds: [41, 42, 43, 44, 45]

# Note: Stage 4 uses frozen threshold from the Stage 3 champion during evaluation.
# Use configs/eval/eval_threshold_frozen.yaml for evaluation to lock results.

early_stopping:
  enabled: true
  patience: 2
  min_delta: 0.001
  metric: "f1"
  mode: "max"

threshold_sweep:
  enabled: true
  metric: "f1"
  range: [0.3, 0.7]
  step: 0.01
