# Efficient batch training configuration for encoder models
# Focused hyperparameter sweep with strategic sampling

global:
  max_length: 512
  warmup_ratio: 0.1
  eval_steps: 100
  save_steps: 200
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 25
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  train_path: "data/processed/chef_train.jsonl"
  val_path: "data/processed/chef_val.jsonl"
  fp16: true

experiments:
  # Smaller models: fewer experiments since they're less critical
  codebert_base:
    model_name: "microsoft/codebert-base"
    hyperparameters:
      learning_rates: [2e-5, 5e-5] # Skip 1e-5 (often too conservative)
      batch_sizes: [16, 32] # Focus on larger batch sizes
      num_epochs: [3, 5] # Skip 7 epochs
      weight_decay: [0.01] # Single weight decay value
    # 2 × 2 × 2 × 1 = 8 experiments

  codet5_small:
    model_name: "Salesforce/codet5-small"
    hyperparameters:
      learning_rates: [2e-5, 5e-5]
      batch_sizes: [16, 32]
      num_epochs: [3, 5]
      weight_decay: [0.01]
    # 8 experiments

  # Medium models: moderate exploration
  codet5_base:
    model_name: "Salesforce/codet5-base"
    hyperparameters:
      learning_rates: [1e-5, 2e-5, 5e-5]
      batch_sizes: [8, 16]
      num_epochs: [3, 5]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1] # Remove multiple accumulation options
    # 3 × 2 × 2 × 1 × 1 = 12 experiments

  codet5p_220m:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      learning_rates: [1e-5, 2e-5, 5e-5]
      batch_sizes: [8, 16]
      num_epochs: [3, 5]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 12 experiments

  # Larger models: more thorough exploration (these matter most)
  codet5_large:
    model_name: "Salesforce/codet5-large"
    hyperparameters:
      learning_rates: [1e-5, 2e-5, 5e-5]
      batch_sizes: [4, 8]
      num_epochs: [3, 5]
      weight_decay: [0.01, 0.1]
      gradient_accumulation_steps: [2] # Fixed accumulation
    gradient_checkpointing: true
    # 3 × 2 × 2 × 2 × 1 = 24 experiments

  codet5p_770m:
    model_name: "Salesforce/codet5p-770m"
    hyperparameters:
      learning_rates: [1e-5, 2e-5]
      batch_sizes: [4, 8]
      num_epochs: [3, 5]
      weight_decay: [0.01, 0.1]
      gradient_accumulation_steps: [2]
    gradient_checkpointing: true
    # 2 × 2 × 2 × 2 × 1 = 16 experiments

  codet5p_2b:
    model_name: "Salesforce/codet5p-2b"
    hyperparameters:
      learning_rates: [1e-5, 2e-5]
      batch_sizes: [2, 4]
      num_epochs: [3, 5]
      weight_decay: [0.01, 0.1]
      gradient_accumulation_steps: [4] # Fixed accumulation
    gradient_checkpointing: true
    # 2 × 2 × 2 × 2 × 1 = 16 experiments

# Early stopping to prevent wasted compute
early_stopping:
  patience: 2
  min_delta: 0.001

output:
  base_dir: "models/experiments/encoder"
  results_dir: "results/experiments/encoder"
  naming_pattern: "{model}_{lr}_{bs}_{epochs}_{wd}"
