# Batch evaluation configuration for encoder models only
# Simplified and focused - no generative or comparison features

# Core evaluation settings
evaluation:
  # Global settings
  batch_size: 8 # Default batch size for encoder models
  max_samples: null # null for full dataset, set number for testing
  save_predictions: true

  # Data configuration - support both combined and technology-specific test sets
  test_sets:
    combined: "data/processed/test.jsonl"
    technologies:
      chef: "data/processed/chef/chef_test.jsonl"
      ansible: "data/processed/ansible/ansible_test.jsonl"
      puppet: "data/processed/puppet/puppet_test.jsonl"

  output_dir: "results/experiments/evaluation"

  # Metrics to compute (encoder-specific)
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - confusion_matrix
    - per_smell_metrics

# Encoder model configurations (will be dynamically set by batch evaluator)
models:
  encoder:
    path: "models/encoder_latest" # Will be overridden per model
    batch_size: 8 # Will be resolved based on model type

# Batch processing settings
batch_evaluation:
  job_spacing_seconds: 1 # Spacing between job submissions to avoid SLURM overload
