# Focused batch training configuration for codet5p-220m and codet5p-770m only
# Optimized hyperparameter sweep based on previous results

global:
  max_length: 512
  warmup_ratio: 0.1
  eval_steps: 100
  save_steps: 200
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 25
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  train_path: "data/processed/train.jsonl"
  val_path: "data/processed/val.jsonl"
  fp16: true
  dataloader_pin_memory: true

experiments:
  codet5p_220m:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      # Expanded LR sweep around 5e-5 (best found in Round 1)
      learning_rates: [3e-5, 4e-5, 5e-5, 6e-5, 8e-5]
      batch_sizes: [8] # Fixed at 8 (best performing)
      num_epochs: [5, 7] # Extend slightly, since 220M is lighter
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 5 × 1 × 2 × 1 × 1 = 10 experiments

  codet5p_770m:
    model_name: "Salesforce/codet5p-770m"
    hyperparameters:
      # Expanded LR sweep around 2e-5 (best found in Round 1)
      learning_rates: [1.5e-5, 2e-5, 2.5e-5, 3e-5]
      batch_sizes: [4] # Fixed at 4 (best performing)
      num_epochs: [5] # Keep stable, avoids overfit
      weight_decay: [0.1] # Best seen in Round 1
      gradient_accumulation_steps: [2]
    gradient_checkpointing: true
    # 4 × 1 × 1 × 1 × 1 = 4 experiments

# Early stopping to prevent wasted compute
early_stopping:
  patience: 2
  min_delta: 0.001

output:
  base_dir: "models/experiments/encoder"
  results_dir: "results/experiments/encoder"
  naming_pattern: "{model}_{lr}_{bs}_{epochs}_{wd}"
