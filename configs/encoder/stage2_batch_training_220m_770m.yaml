# Focused batch training configuration for codet5p-220m and codet5p-770m only
# Global defaults provided via src/utils/config_defaults.py

experiments:
  codet5p_220m:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      # Expanded LR sweep around 5e-5 (best found in Round 1)
      learning_rates: [3e-5, 4e-5, 5e-5, 6e-5, 8e-5]
      batch_sizes: [8] # Fixed at 8 (best performing)
      num_epochs: [5, 7] # Extend slightly, since 220M is lighter
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 5 × 1 × 2 × 1 × 1 = 10 experiments

  codet5p_770m:
    model_name: "Salesforce/codet5p-770m"
    hyperparameters:
      # Expanded LR sweep around 2e-5 (best found in Round 1)
      learning_rates: [1.5e-5, 2e-5, 2.5e-5, 3e-5]
      batch_sizes: [4] # Fixed at 4 (best performing)
      num_epochs: [5] # Keep stable, avoids overfit
      weight_decay: [0.1] # Best seen in Round 1
      gradient_accumulation_steps: [2]
    gradient_checkpointing: true
    # 4 × 1 × 1 × 1 × 1 = 4 experiments

# Early stopping defaults apply automatically (patience=2, min_delta=0.001)
