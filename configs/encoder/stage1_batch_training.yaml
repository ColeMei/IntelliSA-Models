# Efficient batch training configuration for encoder models
# Common globals/outputs inherit from src/utils/config_defaults.py

experiments:
  # Smaller models: fewer experiments since they're less critical
  codebert_base:
    model_name: "microsoft/codebert-base"
    hyperparameters:
      learning_rates: [2e-5, 5e-5] # Skip 1e-5 (often too conservative)
      batch_sizes: [16, 32] # Focus on larger batch sizes
      num_epochs: [3, 5] # Skip 7 epochs
      weight_decay: [0.01] # Single weight decay value
    # 2 × 2 × 2 × 1 = 8 experiments

  codet5_small:
    model_name: "Salesforce/codet5-small"
    hyperparameters:
      learning_rates: [2e-5, 5e-5]
      batch_sizes: [16, 32]
      num_epochs: [3, 5]
      weight_decay: [0.01]
    # 8 experiments

  # Medium models: moderate exploration
  codet5_base:
    model_name: "Salesforce/codet5-base"
    hyperparameters:
      learning_rates: [1e-5, 2e-5, 5e-5]
      batch_sizes: [8, 16]
      num_epochs: [3, 5]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1] # Remove multiple accumulation options
    # 3 × 2 × 2 × 1 × 1 = 12 experiments

  codet5p_220m:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      learning_rates: [1e-5, 2e-5, 5e-5]
      batch_sizes: [8, 16]
      num_epochs: [3, 5]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 12 experiments

  # Larger models: more thorough exploration (these matter most)
  codet5_large:
    model_name: "Salesforce/codet5-large"
    hyperparameters:
      learning_rates: [1e-5, 2e-5]
      batch_sizes: [4, 8]
      num_epochs: [3, 5]
      weight_decay: [0.01, 0.1]
      gradient_accumulation_steps: [2] # Fixed accumulation
    gradient_checkpointing: true
    # 2 × 2 × 2 × 2 × 1 = 16 experiments

  codet5p_770m:
    model_name: "Salesforce/codet5p-770m"
    hyperparameters:
      learning_rates: [1e-5, 2e-5]
      batch_sizes: [4, 8]
      num_epochs: [3, 5]
      weight_decay: [0.01, 0.1]
      gradient_accumulation_steps: [2]
    gradient_checkpointing: true
    # 2 × 2 × 2 × 2 × 1 = 16 experiments

# Early stopping defaults apply automatically (patience=2, min_delta=0.001)
