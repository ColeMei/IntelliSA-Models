# Final sweep configuration for codet5p-220m
# Focused hyperparameter optimization with advanced features

global:
  max_length: 512
  eval_steps: 100
  save_steps: 200
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 25
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  train_path: "data/processed/train.jsonl"
  val_path: "data/processed/val.jsonl"
  fp16: true
  dataloader_pin_memory: true
  lr_scheduler_type: "cosine"
  warmup_steps: 500

experiments:
  codet5p_220m_final_sweep:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      # Both top contenders from previous rounds
      learning_rates: [4e-5, 5e-5]
      batch_sizes: [8] # Fixed at 8 (best performing)
      num_epochs: [6, 7, 8] # Extended range for final optimization
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 2 × 1 × 3 × 1 × 1 = 6 experiments

# Enhanced early stopping configuration
early_stopping:
  enabled: true
  patience: 2
  min_delta: 0.001
  metric: "f1"
  mode: "max"

# Threshold sweep configuration for final optimization
threshold_sweep:
  enabled: true
  metric: "f1"
  range: [0.3, 0.7]
  step: 0.01

output:
  base_dir: "models/experiments/encoder"
  results_dir: "results/experiments/encoder"
  naming_pattern: "{model}_{lr}_{bs}_{epochs}_{wd}"
