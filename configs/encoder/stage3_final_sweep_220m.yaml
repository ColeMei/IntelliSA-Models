# Final sweep configuration for codet5p-220m
# Shared defaults come from src/utils/config_defaults.py

global:
  lr_scheduler_type: "cosine"
  warmup_steps: 500

experiments:
  codet5p_220m_final_sweep:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      # Both top contenders from previous rounds
      learning_rates: [4e-5, 5e-5]
      batch_sizes: [8] # Fixed at 8 (best performing)
      num_epochs: [6, 7, 8] # Extended range for final optimization
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
    # 2 × 1 × 3 × 1 × 1 = 6 experiments

# Enhanced early stopping configuration
early_stopping:
  enabled: true
  patience: 2
  min_delta: 0.001
  metric: "f1"
  mode: "max"

# Threshold sweep configuration for final optimization
threshold_sweep:
  enabled: true
  metric: "f1"
  range: [0.3, 0.7]
  step: 0.01
