# Champion batch training configuration for codet5p-220m
# Purpose: run stable multiple seeds on the champion hyperparameters

global:
  max_length: 512
  lr_scheduler_type: "cosine"
  warmup_steps: 500
  eval_steps: 100
  save_steps: 200
  evaluation_strategy: "steps"
  save_strategy: "steps"
  logging_steps: 25
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true
  train_path: "data/processed/train.jsonl"
  val_path: "data/processed/val.jsonl"
  fp16: true
  dataloader_pin_memory: true

experiments:
  codet5p_220m_champion:
    model_name: "Salesforce/codet5p-220m"
    hyperparameters:
      learning_rates: [4e-5]
      batch_sizes: [8]
      num_epochs: [6]
      weight_decay: [0.01]
      gradient_accumulation_steps: [1]
      seeds: [41, 42, 43, 44, 45]

early_stopping:
  enabled: true
  patience: 2
  min_delta: 0.001
  metric: "f1"
  mode: "max"

threshold_sweep:
  enabled: true
  metric: "f1"
  range: [0.3, 0.7]
  step: 0.01

output:
  base_dir: "models/experiments/encoder"
  results_dir: "results/experiments/encoder"
  naming_pattern: "{model}_{lr}_{bs}_{epochs}_{wd}"
